{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import pandas as pd #data processing\n",
    "import numpy as np #math\n",
    "\n",
    "import seaborn as sns #data visualisation\n",
    "from matplotlib import pyplot as plt #plotting\n",
    "plt.style.use('ggplot')\n",
    "from mpl_toolkits.mplot3d import Axes3D #make the 3D\n",
    "from sklearn.model_selection import train_test_split #split the model\n",
    "\n",
    "#data processing\n",
    "\n",
    "from sklearn.utils import resample\n",
    "from imblearn.over_sampling import SMOTE #upsampling - SMOTE\n",
    "from sklearn import preprocessing #data preprocessing - normalize/standardize\n",
    "\n",
    "#models\n",
    "from sklearn.linear_model import LogisticRegression #logistic regression\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.ensemble import RandomForestClassifier #random forest classifier\n",
    "from sklearn.svm import SVC #svm\n",
    "\n",
    "#evaluation\n",
    "from sklearn.metrics import roc_auc_score #area under ROC\n",
    "import statsmodels.api as sm\n",
    "\n",
    "\n",
    "# import warnings filter\n",
    "from warnings import simplefilter\n",
    "# ignore all future warnings\n",
    "simplefilter(action='ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    " #i/o and data preparation\n",
    "card = pd.read_csv(\"card.csv\", header = 1, index_col = 0)\n",
    "\n",
    "default = card[card['default payment next month'] == 1]\n",
    "no_default = card[card['default payment next month'] == 0]\n",
    "\n",
    "#categorical variables: SEX, EDUCATION, MARRIAGE, PAY_0, ..., PAY_6\n",
    "#make these variables into categorical\n",
    "card[\"SEX\"] = card[\"SEX\"].astype('category')\n",
    "card[\"EDUCATION\"] = card[\"EDUCATION\"].astype('category')\n",
    "card[\"MARRIAGE\"] = card[\"MARRIAGE\"].astype('category')\n",
    "card[\"PAY_0\"] = card[\"PAY_0\"].astype('category')\n",
    "card[\"PAY_2\"] = card[\"PAY_2\"].astype('category')\n",
    "card[\"PAY_3\"] = card[\"PAY_3\"].astype('category')\n",
    "card[\"PAY_4\"] = card[\"PAY_4\"].astype('category')\n",
    "card[\"PAY_5\"] = card[\"PAY_5\"].astype('category')\n",
    "card[\"PAY_6\"] = card[\"PAY_6\"].astype('category')\n",
    "#card[\"default payment next month\"] = card[\"default payment next month\"].astype('category')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split up the data frame into rich and poor, with rich being those with LIMIT_BAL > 300000\n",
    "rich = card[card[\"LIMIT_BAL\"] > 300000]\n",
    "poor = card.loc[~card.index.isin(rich.index),:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#frequency\n",
    "for i in rich,poor:\n",
    "    defaults = pd.value_counts(i['default payment next month'], sort = True)\n",
    "    ax = defaults.plot(kind = 'bar', rot = 0, color = ['coral','b'])\n",
    "    # create a list to collect the plt.patches data\n",
    "    totals = []\n",
    "\n",
    "    # find the values and append to list\n",
    "    for j in ax.patches:\n",
    "        totals += [j.get_height(),]\n",
    "\n",
    "    # set individual bar lables using above list\n",
    "    total = sum(totals)\n",
    "\n",
    "    # set individual bar lables using above list\n",
    "    for j in ax.patches:\n",
    "        # get_x pulls left or right; get_height pushes up or down\n",
    "        ax.text(j.get_x() + 0.1, j.get_height(), \\\n",
    "                str(round((j.get_height()/total)*100, 2))+'%', fontsize=15,\n",
    "                    color='dimgrey')\n",
    "\n",
    "    \n",
    "    plt.title(\"Default for \" + ('rich' if i.equals(rich) else 'poor') + ' people, amount = ' + str(len(i)))\n",
    "    \n",
    "    plt.xlabel(\"Default\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#correlation matrix\n",
    "\n",
    "for i in rich,poor:\n",
    "    label = \"rich\" if i.equals(rich) else 'poor'\n",
    "    default, no_default = i[i['default payment next month'] == 1],i[i['default payment next month'] == 0]\n",
    "    for j in default,no_default:\n",
    "        default_status = \"default\" if j.equals(default) else \"do not default\" \n",
    "        correlation_matrix = j.corr(method = 'pearson')\n",
    "        fig = plt.figure(figsize=(12,9))\n",
    "        plt.title(\"Correlation matrix for \" + label + \" people who \" + default_status)\n",
    "        mask = np.zeros_like(correlation_matrix, dtype=np.bool)\n",
    "        mask[np.triu_indices_from(mask)] = True\n",
    "        sns.heatmap(correlation_matrix,vmax=1, vmin = -1, cmap = 'coolwarm', annot = True, mask = mask, square = True)\n",
    "        plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rich\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.614139\n",
      "         Iterations 7\n",
      "Accuracy of logistic regression classifier on test set: 0.63\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.63      0.75      2637\n",
      "           1       0.22      0.68      0.33       405\n",
      "\n",
      "    accuracy                           0.63      3042\n",
      "   macro avg       0.57      0.65      0.54      3042\n",
      "weighted avg       0.83      0.63      0.69      3042\n",
      "\n",
      "poor\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.669164\n",
      "         Iterations 5\n",
      "Accuracy of logistic regression classifier on test set: 0.59\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.61      0.70     12989\n",
      "           1       0.30      0.54      0.38      3969\n",
      "\n",
      "    accuracy                           0.59     16958\n",
      "   macro avg       0.55      0.57      0.54     16958\n",
      "weighted avg       0.69      0.59      0.62     16958\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#do logistic regression\n",
    "\n",
    "for i in rich,poor:\n",
    "    label = 'rich' if i.equals(rich) else 'poor'\n",
    "    \n",
    "    features = i.drop(['default payment next month'], axis = 1)\n",
    "\n",
    "    feature_names = features.columns\n",
    "    \n",
    "    #normalize the features\n",
    "    features = preprocessing.normalize(features)\n",
    "    features = pd.DataFrame(data = features, columns = feature_names)\n",
    "    #do we need to standardize though?\n",
    "    \n",
    "    #split the data and normalize it\n",
    "    train_features, test_features,\\\n",
    "    train_target, test_target = train_test_split(features, i['default payment next month'],\n",
    "                                                 test_size = 2/3, random_state = 123)\n",
    "    \n",
    "    #split into the validation set    \n",
    "    x_train, x_val, y_train, y_val = train_test_split(train_features, train_target,\n",
    "                                                  test_size = .2,\n",
    "                                                  random_state=123) #20-80 split for validation set\n",
    "    \n",
    "    \n",
    "    \n",
    "    #do resampling -- upsampling using SMOTE\n",
    "    #we will not be doing downsampling because the training data set will become quite small.\n",
    "    x_train_res, y_train_res = SMOTE(random_state = 123).fit_sample(x_train, y_train)\n",
    "    \n",
    "    x_train = pd.DataFrame(data = x_train_res, columns = feature_names)\n",
    "    y_train = pd.DataFrame(data = y_train_res, columns = [\"default payment next month\"])\n",
    "\n",
    "    \n",
    "    #model: logistic regression\n",
    "    logreg = LogisticRegression(random_state = 123, solver = 'lbfgs')\n",
    "        \n",
    "    #feature selection -- using Recursive Feature Elminiation with Cross Validation\n",
    "    rfecv = RFECV(estimator=logreg, step=1, cv=StratifiedKFold(10)\n",
    "                                   ,scoring='accuracy') #this can be changed to determine which feature is best\n",
    "    rfecv.fit(x_train, y_train.values.ravel())\n",
    "\n",
    "    \n",
    "\n",
    "    print(label)\n",
    "    #print('Optimal number of features: {}'.format(rfecv.n_features_))\n",
    "\n",
    "    ranking = rfecv.ranking_\n",
    "    headings = list(card)\n",
    "    ranking_index = [i for i in range(len(ranking)) if ranking[i] == 1]\n",
    "\n",
    "    selected_features = [headings[i] for i in ranking_index]\n",
    "    \n",
    "    #we only want to look at these features\n",
    "    x = x_train[selected_features]\n",
    "       \n",
    "    logit_model=sm.Logit(y_train,x)\n",
    "    result=logit_model.fit()\n",
    "\n",
    "    p_values = result.pvalues\n",
    "    features_to_keep = [i for i in range(len(p_values)) if p_values[i] < 0.05] #keep those with p-values smaller than 0.05\n",
    "    selected_features = [selected_features[i] for i in features_to_keep]\n",
    "\n",
    "    \n",
    "    x = x_train[selected_features]\n",
    "    test_features = test_features[selected_features]\n",
    "    \n",
    "    #the model\n",
    "    logreg = LogisticRegression(random_state = 123, solver = 'lbfgs').fit(x, y_train.values.ravel())\n",
    "    prediction = logreg.predict(test_features)\n",
    "    print('Accuracy of logistic regression classifier on test set: {:.2f}'.format(logreg.score(test_features, test_target)))\n",
    "        \n",
    "    from sklearn.metrics import classification_report\n",
    "    print(classification_report(test_target, prediction))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rich\n",
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 12.447000\n",
      "         Iterations: 35\n",
      "Accuracy of random forest classifier on test set: 0.80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\joobi\\Anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py:512: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  \"Check mle_retvals\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.87      0.88      2637\n",
      "           1       0.28      0.32      0.30       405\n",
      "\n",
      "    accuracy                           0.80      3042\n",
      "   macro avg       0.59      0.60      0.59      3042\n",
      "weighted avg       0.81      0.80      0.81      3042\n",
      "\n",
      "poor\n",
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: inf\n",
      "         Iterations: 35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\joobi\\Anaconda3\\lib\\site-packages\\statsmodels\\discrete\\discrete_model.py:1789: RuntimeWarning: divide by zero encountered in log\n",
      "  return np.sum(np.log(self.cdf(q*np.dot(X,params))))\n",
      "C:\\Users\\joobi\\Anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py:512: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  \"Check mle_retvals\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of random forest classifier on test set: 0.77\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.87      0.85     12989\n",
      "           1       0.52      0.45      0.48      3969\n",
      "\n",
      "    accuracy                           0.77     16958\n",
      "   macro avg       0.68      0.66      0.67     16958\n",
      "weighted avg       0.76      0.77      0.77     16958\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#do random forest classifier\n",
    "for i in rich,poor:\n",
    "    label = 'rich' if i.equals(rich) else 'poor'\n",
    "    \n",
    "    features = i.drop(['default payment next month'], axis = 1)\n",
    "\n",
    "    feature_names = features.columns\n",
    "    \n",
    "    #normalize the features\n",
    "    features = preprocessing.normalize(features)\n",
    "    features = pd.DataFrame(data = features, columns = feature_names)\n",
    "    #do we need to standardize though?\n",
    "    \n",
    "    #split the data and normalize it\n",
    "    train_features, test_features,\\\n",
    "    train_target, test_target = train_test_split(features, i['default payment next month'],\n",
    "                                                 test_size = 2/3, random_state = 123)\n",
    "    \n",
    "    #split into the validation set    \n",
    "    x_train, x_val, y_train, y_val = train_test_split(train_features, train_target,\n",
    "                                                  test_size = .2,\n",
    "                                                  random_state=123) #20-80 split for validation set\n",
    "    \n",
    "    \n",
    "    \n",
    "    #do resampling -- upsampling using SMOTE\n",
    "    #we will not be doing downsampling because the training data set will become quite small.\n",
    "    x_train_res, y_train_res = SMOTE(random_state = 123).fit_sample(x_train, y_train)\n",
    "    \n",
    "    x_train = pd.DataFrame(data = x_train_res, columns = feature_names)\n",
    "    y_train = pd.DataFrame(data = y_train_res, columns = [\"default payment next month\"])\n",
    "\n",
    "    \n",
    "    #model: random forest classifier\n",
    "    rfc = RandomForestClassifier(n_estimators = 10, criterion = 'gini', random_state = 123)\n",
    "        \n",
    "    #feature selection -- using Recursive Feature Elminiation with Cross Validation\n",
    "    rfecv = RFECV(estimator=rfc, step=1, cv=StratifiedKFold(10)\n",
    "                                   ,scoring='accuracy') #this can be changed to determine which feature is best\n",
    "    rfecv.fit(x_train, y_train.values.ravel())\n",
    "\n",
    "    \n",
    "\n",
    "    print(label)\n",
    "    #print('Optimal number of features: {}'.format(rfecv.n_features_))\n",
    "\n",
    "    ranking = rfecv.ranking_\n",
    "    headings = list(card)\n",
    "    ranking_index = [i for i in range(len(ranking)) if ranking[i] == 1]\n",
    "\n",
    "    selected_features = [headings[i] for i in ranking_index]\n",
    "    \n",
    "    #we only want to look at these features\n",
    "    x = x_train[selected_features]\n",
    "       \n",
    "    logit_model=sm.Logit(y_train,x)\n",
    "    result=logit_model.fit()\n",
    "\n",
    "    p_values = result.pvalues\n",
    "    features_to_keep = [i for i in range(len(p_values)) if p_values[i] < 0.05] #keep those with p-values smaller than 0.05\n",
    "    selected_features = [selected_features[i] for i in features_to_keep]\n",
    "\n",
    "    \n",
    "    x = x_train[selected_features]\n",
    "    test_features = test_features[selected_features]\n",
    "    \n",
    "    #the model\n",
    "    rfc = RandomForestClassifier(n_estimators = 10, criterion = 'gini', random_state = 123).fit(x, y_train.values.ravel())\n",
    "    prediction = rfc.predict(test_features)\n",
    "    print('Accuracy of random forest classifier on test set: {:.2f}'.format(rfc.score(test_features, test_target)))\n",
    "        \n",
    "    from sklearn.metrics import classification_report\n",
    "    print(classification_report(test_target, prediction))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rich\n",
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 12.447000\n",
      "         Iterations: 35\n",
      "Accuracy of support vector machine on test set: 0.76\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.85      0.86      2637\n",
      "           1       0.19      0.23      0.21       405\n",
      "\n",
      "    accuracy                           0.76      3042\n",
      "   macro avg       0.53      0.54      0.54      3042\n",
      "weighted avg       0.79      0.76      0.78      3042\n",
      "\n",
      "poor\n",
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: inf\n",
      "         Iterations: 35\n",
      "Accuracy of support vector machine on test set: 0.55\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.54      0.64     12989\n",
      "           1       0.28      0.58      0.38      3969\n",
      "\n",
      "    accuracy                           0.55     16958\n",
      "   macro avg       0.54      0.56      0.51     16958\n",
      "weighted avg       0.68      0.55      0.58     16958\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#do support vector machine\n",
    "for i in rich,poor:\n",
    "    label = 'rich' if i.equals(rich) else 'poor'\n",
    "    \n",
    "    features = i.drop(['default payment next month'], axis = 1)\n",
    "\n",
    "    feature_names = features.columns\n",
    "    \n",
    "    #normalize the features\n",
    "    features = preprocessing.normalize(features)\n",
    "    features = pd.DataFrame(data = features, columns = feature_names)\n",
    "    #do we need to standardize though?\n",
    "    \n",
    "    #split the data and normalize it\n",
    "    train_features, test_features,\\\n",
    "    train_target, test_target = train_test_split(features, i['default payment next month'],\n",
    "                                                 test_size = 2/3, random_state = 123)\n",
    "    \n",
    "    #split into the validation set    \n",
    "    x_train, x_val, y_train, y_val = train_test_split(train_features, train_target,\n",
    "                                                  test_size = .2,\n",
    "                                                  random_state=123) #20-80 split for validation set\n",
    "    \n",
    "    \n",
    "    \n",
    "    #do resampling -- upsampling using SMOTE\n",
    "    #we will not be doing downsampling because the training data set will become quite small.\n",
    "    x_train_res, y_train_res = SMOTE(random_state = 123).fit_sample(x_train, y_train)\n",
    "    \n",
    "    x_train = pd.DataFrame(data = x_train_res, columns = feature_names)\n",
    "    y_train = pd.DataFrame(data = y_train_res, columns = [\"default payment next month\"])\n",
    "\n",
    "    \n",
    "    #model: random forest classifier\n",
    "    rfc = RandomForestClassifier(n_estimators = 10, criterion = 'gini', random_state = 123)\n",
    "        \n",
    "    #feature selection -- using Recursive Feature Elminiation with Cross Validation\n",
    "    rfecv = RFECV(estimator=rfc, step=1, cv=StratifiedKFold(10)\n",
    "                                   ,scoring='accuracy') #this can be changed to determine which feature is best\n",
    "    rfecv.fit(x_train, y_train)\n",
    "\n",
    "    \n",
    "\n",
    "    print(label)\n",
    "    #print('Optimal number of features: {}'.format(rfecv.n_features_))\n",
    "\n",
    "    ranking = rfecv.ranking_\n",
    "    headings = list(card)\n",
    "    ranking_index = [i for i in range(len(ranking)) if ranking[i] == 1]\n",
    "\n",
    "    selected_features = [headings[i] for i in ranking_index]\n",
    "    \n",
    "    #we only want to look at these features\n",
    "    x = x_train[selected_features]\n",
    "       \n",
    "    logit_model=sm.Logit(y_train,x)\n",
    "    result=logit_model.fit()\n",
    "\n",
    "    p_values = result.pvalues\n",
    "    features_to_keep = [i for i in range(len(p_values)) if p_values[i] < 0.05] #keep those with p-values smaller than 0.05\n",
    "    selected_features = [selected_features[i] for i in features_to_keep]\n",
    "\n",
    "    \n",
    "    x = x_train[selected_features]\n",
    "    test_features = test_features[selected_features]\n",
    "    \n",
    "    #the model\n",
    "    svm = SVC(kernel = 'rbf',  random_state = 123).fit(x, y_train)\n",
    "    prediction = svm.predict(test_features)\n",
    "    print('Accuracy of support vector machine on test set: {:.2f}'.format(svm.score(test_features, test_target)))\n",
    "        \n",
    "    from sklearn.metrics import classification_report\n",
    "    print(classification_report(test_target, prediction))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary statistics, we do this to look out for anomalies in the test/training data, so that we can be careful during learning\n",
    "#for testing data\n",
    "test.describe()\n",
    "#see the skewness \n",
    "#print(test.skew(axis = 0,numeric_only = True))\n",
    "#print(test.kurt(axis = 0,numeric_only = True))\n",
    "\n",
    "for i in test:\n",
    "    \n",
    "    if not hasattr(test[i],'cat'):\n",
    "        #plot histogram\n",
    "        plt.hist(test[i], bins = 50)\n",
    "        plt.title(i)\n",
    "        plt.ylabel(\"Number of people\")\n",
    "        plt.xlabel(\"Amount\")\n",
    "        plt.show()\n",
    "        #plot box and whisker plot\n",
    "        # plt.boxplot(test[i])\n",
    "        #plt.title(i)\n",
    "        #plt.ylabel(\"Number of people\")\n",
    "        #plt.xlabel(\"Amount\")\n",
    "        #plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(data[\"\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "months = [\"PAY_0\", \"PAY_2\", \"PAY_3\", \"PAY_4\", \"PAY_5\", \"PAY_6\"]\n",
    "\n",
    "for month in months:\n",
    "    a = test[month].unique()\n",
    "    a.sort()\n",
    "    for i in a:\n",
    "        plt.figure(figsize = (10,10))\n",
    "        #ax = fig.add_subplot(111, projection='3d')\n",
    "        data = test[test[month]==i]\n",
    "\n",
    "        #defaulter bitch\n",
    "        plt.scatter(data[\"BILL_AMT1\"][data['default payment next month'] == 1],\n",
    "                    data[\"PAY_AMT1\"][data['default payment next month'] == 1],\n",
    "                    c = 'red',label = '1', marker = '.')\n",
    "        plt.xlabel('BILL_AMT1')\n",
    "        plt.ylabel('PAY_AMT1')\n",
    "\n",
    "        #no default pro\n",
    "        plt.scatter(data[\"BILL_AMT1\"][data['default payment next month'] == 0],\n",
    "                    data[\"PAY_AMT1\"][data['default payment next month'] == 0],\n",
    "                    c = 'blue',label = '0', marker = '+', alpha = 0.3)\n",
    "        plt.xlabel('BILL_AMT1')\n",
    "        plt.ylabel('PAY_AMT1')\n",
    "\n",
    "        plt.title(month + \"= \" + str(i))\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
